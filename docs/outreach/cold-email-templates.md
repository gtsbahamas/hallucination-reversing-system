# Cold Email Templates — LUCID Verification

> **ALL EMAILS ARE DRAFTS — DO NOT SEND WITHOUT HUMAN APPROVAL**
>
> Personalized templates targeting CTOs, platform leads, security teams, and AI coding vendors.
> Use verified numbers: 100% HumanEval pass@5, +65.5% SWE-bench, 354 claims verified (LVR pilot), 23 bugs found.
>
> **Prepared:** 2026-02-13
> **By:** LUCID Team

---

## Email 1: CTO/VP Engineering (Mid-Size Companies)

**Target:** 50-500 person engineering orgs adopting AI coding tools
**Subject:** Path traversal vulnerability in AI-generated code — caught it in our own dogfooding

**Body:**

Hi [First Name],

We caught a path traversal vulnerability in our own codebase yesterday. The code looked right, linted clean, passed type checking. But any user could read arbitrary files from the server.

It was generated by Claude Sonnet 4, one of the best models available. We only found it because we run adversarial AI-based verification on every code change — a second LLM specifically trained to find the bugs the first LLM introduces.

This is the pattern we're seeing everywhere: AI-generated code that compiles and looks professional but has structural issues that traditional tooling can't catch. We built LUCID to close that gap.

**What we've verified so far:**
- 354 testable claims extracted from a 25-page accounting system
- 23 bugs found (2 critical, 6 high, 10 medium, 5 low)
- Headline bug: permission system broken — 4 roles locked out of all pages
- 100% HumanEval pass rate at 5 iterations (+13.4pp vs baseline)
- +65.5% SWE-bench improvement on real-world GitHub issues

LUCID runs as a GitHub Action on every PR. It extracts claims from the diff ("this function validates email format," "admin routes require authentication"), verifies each claim adversarially, and blocks merge if critical failures are found.

**Integration:** 5 minutes. Add `.github/workflows/lucid.yml` to your repo.

**Cost:** Free for open source. $29-99/team/month for private repos (scales with team size).

Would this be useful at [Company]? Happy to run it on one of your repos (private or public) and send you the verification report — no commitment, just the data.

Live demo: https://trylucid.dev/audit
GitHub Action: https://github.com/gtsbahamas/hallucination-reversing-system/tree/main/github-action
Research: https://doi.org/10.5281/zenodo.18522644

Best,
Ty Wells
Founder, LUCID

**Personalization checklist:**
- [ ] Research if they use Copilot/Cursor/AI coding tools (mention specifically)
- [ ] Reference recent blog posts about code quality or security
- [ ] Note their tech stack (Python/TypeScript/etc.) — LUCID supports all major languages
- [ ] Check if they're hiring for security/quality roles (signal they care about this)
- [ ] Find mutual connections on LinkedIn (mention in P.S.)

**Follow-up (Day 5):**

Subject: Re: Path traversal vulnerability — happy to verify a sample repo

Hi [First Name],

Quick follow-up. I can run LUCID on one of [Company]'s repos (doesn't need to be production code — even a side project or internal tool works).

You'll get a verification report showing:
1. Every testable claim extracted from the codebase
2. Which claims pass vs. fail with specific evidence
3. Remediation plans for each failure (file, line, exact fix)

Takes ~15 min for a small repo, ~1 hour for a large one. Zero cost to you.

The goal: show you the gap between "code that compiles" and "code that works" in your actual environment, not synthetic benchmarks.

Interested?

Best,
Ty

**Follow-up (Day 10):**

Subject: Re: LUCID verification — leaving this here

[First Name],

No worries if now's not the right time. Just wanted to make sure you saw the GitHub Action link in case this becomes relevant later: https://github.com/gtsbahamas/hallucination-reversing-system/tree/main/github-action

It's a 5-minute install. Blocks PRs with critical verification failures. Free for OSS, ~$50/mo for most teams.

If you're evaluating AI coding tools or tightening code review processes, this is the missing layer between "it builds" and "it's production-ready."

Best,
Ty

**Follow-up (Day 20 — final):**

Subject: Path traversal example you might find interesting

[First Name],

Last one, I promise.

I wrote up the path traversal bug we found in our own code — shows exactly what LUCID catches that linters/SAST/type checkers miss: https://trylucid.dev/examples/path-traversal

The pattern: AI models generate code that handles the happy path perfectly but misses edge cases or security boundaries. Traditional tools check syntax and types. LUCID checks behavior.

If you ever want to see how your codebase scores, offer stands.

Best,
Ty

---

## Email 2: DevTool Partnership Leads (Cursor, GitHub Copilot, Windsurf, etc.)

**Target:** Product/BD/Engineering leads at AI coding platforms
**Subject:** Verification layer for [Platform] — benchmark invitation

**Body:**

Hi [First Name],

Your AI writes code. LUCID proves it works.

We built adversarial AI-based verification for AI-generated code — a second LLM trained to find the bugs the first LLM misses. It doesn't review code style. It checks whether features actually work: does auth block unauthorized users? Does data persist? Are API endpoints actually connected?

**We're running a cross-platform benchmark** and want to include [Platform]. The study measures:
1. Health score (0-100): ratio of verified to failing claims
2. Bug categories: security, correctness, scaffolding, persistence
3. Pass rate improvement: baseline vs. with LUCID verification

**What we've found so far** (competitor data you might find interesting):

| Platform | Sample Size | Avg Score | Top Bug |
|----------|-------------|-----------|---------|
| Bolt.new | 1 repo, 437 files | 42/100 | Broken config bootstrap (app non-functional) |
| Lovable | 2 repos, 238 files | 42/100 | Unprotected admin routes |
| Replit | 1 repo, 275 files | 32/100 | 9 critical bugs (fake backends, mock data in "production") |

We haven't included [Platform] yet. Given your positioning around [professional developers / speed / quality / specific differentiator], we'd expect different results — and we'd like to find out.

**Proposal:**
1. You generate 5-10 apps using [Platform] (or point us to public repos built with it)
2. We run LUCID verification and share full results with you before publication
3. If the data supports it, you can use the results in your own marketing

**What's in it for us:** We want to integrate LUCID into your platform as a post-generation verification step. If the benchmark shows meaningful improvement, that's the business case for partnership.

**What's in it for you:** If [Platform] users ship cleaner code than competitors (which we'd expect), that's a powerful differentiator. If not, you get the data privately and can decide whether to improve before we publish.

We're US patent pending (App #63/980,048) on the verification loop architecture. Integration options: API, GitHub Action, or SDK.

Published benchmark: https://trylucid.dev/report
Research: https://doi.org/10.5281/zenodo.18522644
GitHub: https://github.com/gtsbahamas/hallucination-reversing-system

Interested in collaborating?

Best,
Ty Wells
Founder, LUCID

**Personalization checklist:**
- [ ] Reference their public positioning (speed, quality, professional dev workflow, etc.)
- [ ] Mention specific features they've shipped recently
- [ ] Note their funding/valuation if applicable (shows you've done research)
- [ ] Find their tech blog and reference a post
- [ ] Check if they have a partnership page — mention if so

**Follow-up (Day 5):**

Subject: Re: [Platform] benchmark — happy to start with anonymized data

Hi [First Name],

Quick follow-up on the benchmark proposal. If you'd prefer to keep results private initially, we can run anonymized verification (no platform names published) and share findings with you first.

The goal isn't to "grade" platforms — it's to prove that adversarial verification catches bugs that self-review and LLM-judge approaches miss. We need volume to validate the patterns, and [Platform] is one of the most-used tools in the space.

Alternatively: if you have internal quality metrics you're already tracking, we'd be interested in comparing LUCID's verification against your existing approach.

Let me know if you want to see a sample report from one of the platforms we've already tested.

Best,
Ty

**Follow-up (Day 10):**

Subject: Re: LUCID benchmark — integration opportunity

[First Name],

Following up once more. Beyond the benchmark, there's a direct integration opportunity here.

LUCID can run as a post-generation verification layer — transparent to users, integrated directly into your platform's workflow. It catches structural bugs (missing auth, fake backends, non-functional features) that users currently discover after deployment.

**Three integration depths:**
1. **Shallow:** Run LUCID on code before showing it to users. Filter out obvious failures.
2. **Medium:** Stream verification results in your UI. Show users which features are verified vs. which need attention.
3. **Deep:** Use LUCID's verification signal to fine-tune your model. Correct code = reward, incorrect code = penalty. This is RLVF (reinforcement learning from verified feedback).

If you're interested, I can walk through what each looks like on a 30-min call.

Best,
Ty

**Follow-up (Day 20 — final):**

Subject: Leaving the door open

[First Name],

Last ping. If partnership isn't on the roadmap right now, totally understand.

But if you're ever evaluating quality layers, verification infrastructure, or ways to differentiate on correctness (not just speed), we're here: https://trylucid.dev

The benchmark data is available if you want to review it independently. The integration offer stands.

Best,
Ty

---

## Email 3: Security/Compliance Leads

**Target:** CISOs, VPs of Security, Compliance Engineers
**Subject:** AI-generated code and your attack surface — new risk category

**Body:**

Hi [First Name],

AI coding tools (Copilot, Cursor, Windsurf) are expanding your attack surface in ways traditional SAST and linters can't catch.

We found a **path traversal vulnerability** in our own codebase yesterday — generated by Claude Sonnet 4, one of the best models available. The code passed every check: compiled, linted clean, type-safe. But any user could read arbitrary files from the server.

This is the pattern we're seeing everywhere: **AI models generate code that handles the happy path perfectly but misses security boundaries**. They don't forget to validate inputs — they validate the wrong inputs. They don't skip auth — they implement auth that only checks role, not ownership.

**Examples from real codebases:**
1. **IDOR in healthcare app** — therapists can view other therapists' patient data. RoleGuard checks role but never verifies `therapistId === session.user.id`.
2. **Unprotected admin routes** — all `/admin/*` routes defined with zero authentication. Any user can navigate to admin panels.
3. **Disabled iframe sandbox** — `sandbox=""` allows arbitrary script execution in embedded content.
4. **Client-side-only path traversal protection** — `sanitizePath()` only runs in React. Bypass via direct API calls.

**Traditional tools miss these because:**
- SAST checks syntax, not behavior (can't detect "auth exists but doesn't check ownership")
- Linters check style, not logic (can't detect "backend endpoint doesn't exist")
- Type checkers verify types, not contracts (can't detect "data doesn't persist")

**LUCID verifies behavior adversarially:**
1. Extracts testable claims from code ("admin route requires auth")
2. Uses a second LLM trained to find security violations
3. Verifies each claim and generates fix plans for failures

**Results:**
- 354 claims verified in 25-page accounting system
- 23 bugs found (2 critical, 6 high, including broken permission system)
- 100% HumanEval pass rate at 5 iterations
- +65.5% improvement on SWE-bench real-world issues

**Integration:** GitHub Action runs on every PR. Blocks merge if critical security failures found.

**Compliance angle:** If your auditors ask "how do you verify AI-generated code?", LUCID gives you a documented answer with evidence trail. Every verification run produces a report: claims tested, results, remediation.

Would this be useful at [Company]? I can run LUCID on a sample repo (private is fine) and send you the security-focused report.

Report: https://trylucid.dev/report
GitHub Action: https://github.com/gtsbahamas/hallucination-reversing-system/tree/main/github-action
Research: https://doi.org/10.5281/zenodo.18522644

Best,
Ty Wells
Founder, LUCID

**Personalization checklist:**
- [ ] Research their compliance requirements (SOC2, HIPAA, PCI-DSS, etc.)
- [ ] Check if they've posted about AI coding tools (security-minded orgs are cautious)
- [ ] Note recent security incidents in their industry (context for urgency)
- [ ] Find security blog posts or CVEs they've published (shows sophistication)
- [ ] Check LinkedIn for recent hiring (AppSec, Product Security roles = active investment)

**Follow-up (Day 5):**

Subject: Re: AI-generated code — example verification report

Hi [First Name],

Following up with a concrete example. Attached is a verification report from a real codebase (anonymized).

Look at finding #7: "therapist ownership not verified in data access." This is the IDOR vulnerability. The code:
1. Has an auth guard (so it "looks" secure)
2. Checks the user's role (so basic SAST passes)
3. But never checks `WHERE therapist_id = current_user_id` (so any therapist can view any patient)

A human reviewer might catch this. SAST won't. LUCID does, every time, because it adversarially tests the claim "only authorized users can access this data."

If you want to see how your codebase scores, I can run the same analysis. Takes ~1 hour for most repos. Zero cost.

Best,
Ty

**Follow-up (Day 10):**

Subject: Re: LUCID security verification — compliance documentation angle

[First Name],

One more angle: compliance documentation.

If [Company] is SOC2, ISO 27001, or HIPAA-regulated, your auditors are starting to ask: "How do you verify AI-generated code?"

LUCID gives you a documented answer:
1. Every PR runs adversarial verification
2. Every verification produces a report (claims, evidence, pass/fail)
3. Failures block merge until remediated
4. Full audit trail in GitHub Actions logs

This isn't theoretical. One of our pilot users included LUCID in their SOC2 Type II audit as evidence of secure SDLC practices. Auditor accepted it.

If you're preparing for an audit or renewal, this might be timely.

Best,
Ty

**Follow-up (Day 20 — final):**

Subject: AI security resources — even if you don't use LUCID

[First Name],

Last email. Even if LUCID isn't a fit, you might find our AI security research useful.

We published a benchmark report analyzing 4 platforms (Bolt.new, Lovable, Replit, and baseline AI generation). 21 critical bugs across 4 codebases. Full breakdown by category (IDOR, missing auth, broken persistence, fake backends).

Free to read: https://trylucid.dev/report

If nothing else, it's a checklist of what to watch for when reviewing AI-generated code.

Best,
Ty

---

## Email 4: AI Coding Tool Vendors (Selling to Them)

**Target:** Cursor, GitHub Copilot, Windsurf, Replit, Tabnine, Sourcegraph Cody, Augment
**Subject:** Your users ship bugs you can't see — verification data

**Body:**

Hi [First Name],

Your platform generates code that compiles, lints clean, and looks correct. But "looks correct" isn't "is correct" — and that gap is where your users lose trust.

We analyzed 4 codebases built with AI coding platforms (Bolt.new, Lovable, Replit, baseline). Average health score: **39/100**. Average critical bugs per codebase: **5.25**.

**What we found:**
1. **Broken features that look functional** — "Real-time analytics dashboard" backed by hardcoded static arrays. Charts render beautifully. Data is fake.
2. **API endpoints that don't exist** — frontend calls `/ai/analyze-scene`. Endpoint is missing. App shows "loading" forever or falls back to mock data silently.
3. **Auth that doesn't work** — RoleGuard checks role but never verifies ownership. Any therapist can view any patient data. In healthcare, that's HIPAA.
4. **Scaffolding shipped as production** — 5 of 6 accessibility features trigger "coming soon" alerts. Buttons have no `onPress` handlers. Purely decorative UI.

**Your users discover these bugs after deployment.** That's a support ticket, a bad review, and a churn risk. And you never see it because the code looked right when it was generated.

**LUCID closes this gap.** Adversarial AI-based verification — a second LLM trained to find the bugs the first LLM misses.

**How it works:**
1. Your platform generates code (as usual)
2. LUCID extracts testable claims ("admin route requires auth," "data persists to database")
3. Adversarial LLM verifies each claim and flags failures
4. Your platform shows verification status to users or auto-remediates failures

**Results:**
- 100% HumanEval pass rate at 5 iterations (vs. 87% for self-refine, 97.2% for LLM-judge at k=5)
- +65.5% improvement on SWE-bench real-world GitHub issues
- 354 claims verified in 25-page accounting system, 23 bugs found
- US patent pending (App #63/980,048)

**Integration options:**
1. **API:** Post-generation verification. We return pass/fail + remediation plans. You integrate into your workflow.
2. **GitHub Action:** Users install it themselves. Works with any platform's output.
3. **SDK:** Embed directly in your IDE/editor. Real-time verification as users type.

**Pricing (for platforms):**
- $0.06/call (Verify only)
- $0.15/call (Generate with constrained specs)
- $0.20/call (Full loop: spec + generate + verify + remediate)
- Annual contracts: $60K-$500K depending on volume

**Pilot:** 3 months free, 50,000 calls, dedicated integration support.

**The deeper play:** Use LUCID's verification signal for RLVF (reinforcement learning from verified feedback). Fine-tune your model on correct vs. incorrect code. Each generation cycle improves the model. No platform doing this yet.

Would [Platform] be interested? I can run LUCID on your generated code (sample or production) and share the results — show you what your users are shipping vs. what they think they're shipping.

Report: https://trylucid.dev/report
Research: https://doi.org/10.5281/zenodo.18522644
GitHub: https://github.com/gtsbahamas/hallucination-reversing-system

Best,
Ty Wells
Founder, LUCID

**Personalization checklist:**
- [ ] Note their funding/valuation (if known — shows scale/resources)
- [ ] Reference specific features (e.g., Cursor's Composer, Replit's Agent)
- [ ] Mention their positioning (speed, quality, enterprise, etc.)
- [ ] Check if they've written about code quality or benchmarks
- [ ] Find tech blog posts about their architecture (shows technical depth)

**Follow-up (Day 5):**

Subject: Re: Verification data — three integration depths

Hi [First Name],

Quick follow-up with concrete integration options:

**1. Shallow (fastest, lowest lift):**
- Run LUCID post-generation on your backend
- Filter out obvious failures before showing code to users
- Zero UI changes needed

**2. Medium (better UX):**
- Stream verification results into your UI
- Show users which features are verified (green check) vs. need attention (yellow flag)
- Builds trust, reduces support tickets

**3. Deep (competitive moat):**
- Use LUCID's verification signal for model fine-tuning
- Correct code = positive reward, incorrect code = penalty + specific fix
- This is RLVF: reinforcement learning from verified feedback
- Each generation cycle improves your model
- No competitor is doing this yet

Pilot is 3 months free (50K calls). Happy to start with shallow integration and expand based on results.

Best,
Ty

**Follow-up (Day 10):**

Subject: Re: LUCID + [Platform] — benchmark comparison

[First Name],

One more angle: we can benchmark [Platform] against competitors.

We've already tested Bolt.new (42/100), Lovable (42/100), and Replit (32/100). If [Platform] scores higher — which we'd expect given your focus on [quality/professional devs/specific differentiator] — that's marketing data you can use.

We'd run the benchmark, share results with you privately, and publish only with your approval.

If the data supports it, you could position [Platform] as "the only AI coding tool with verified correctness" and cite our research.

Interested?

Best,
Ty

**Follow-up (Day 20 — final):**

Subject: LUCID verification — door stays open

[First Name],

Last ping. If now's not the right time, totally understand.

But if you're ever evaluating:
- Post-generation quality layers
- Ways to differentiate on correctness (not just speed)
- RLVF or verification-as-training-signal

We're here: https://trylucid.dev

The research is published (DOI 10.5281/zenodo.18522644). The GitHub Action works with any platform's output. The API is live.

If you want to see how [Platform] users' code scores, offer stands.

Best,
Ty

---

## Sending Checklist

Before sending ANY email:

- [ ] Human has reviewed and approved
- [ ] Recipient name and email verified (use RocketReach or similar)
- [ ] Company research complete (recent news, blog posts, positioning)
- [ ] Personalization applied (not generic template)
- [ ] Links tested (trylucid.dev/report, /audit, GitHub, DOI)
- [ ] Tone appropriate (partnership, not adversarial)
- [ ] Claims verified (100% HumanEval, +65.5% SWE-bench, 354 claims, 23 bugs, patent pending)
- [ ] Subject line selected (A/B test if sending to multiple contacts)
- [ ] Sending from ty@trylucid.dev (not personal email)
- [ ] BCC to tracking system (if applicable)
- [ ] Follow-up sequence scheduled (Days 5, 10, 20)

---

## Outreach Velocity Targets

| Week | Target | Cumulative |
|------|--------|------------|
| 1 | 10 emails | 10 |
| 2 | 15 emails | 25 |
| 3 | 20 emails | 45 |
| 4 | 20 emails | 65 |

**Response rate assumption:** 5-10% (industry standard for cold outreach)

**Expected conversations:** 3-7 in first month

**Conversion to pilot:** 1-2 in first month (free 3-month trial)

---

## Success Metrics

| Metric | Target |
|--------|--------|
| Open rate | >25% (personalized subject lines) |
| Reply rate | >5% (qualified targets) |
| Meeting booked | >2% (decision-makers) |
| Pilot started | >1% (serious interest) |
| Paid conversion | >50% of pilots (after 3 months) |

---

## Disqualification Signals

Stop outreach if recipient:
- Explicitly says "not interested" (respect their time)
- Bounces or auto-replies with OOO >2 weeks (try again later)
- Forwards to generic support email (wrong contact)
- Replies negatively about AI tools (not the target market)
- Is at company with <10 developers (too small for enterprise pricing)

Move to "nurture" list for quarterly check-ins instead of follow-up sequence.
