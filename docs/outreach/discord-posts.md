# Discord Community Posts

> **DRAFT — DO NOT POST WITHOUT HUMAN APPROVAL**
> These are for community channels. Be helpful, not promotional.
> Prepared: 2026-02-11

---

## 1. Cursor Discord

**Channel:** #general or #showcase (check community guidelines first)

---

**Interesting pattern in AI-generated code**

I've been running formal verification on code generated by AI coding tools — not linting or type-checking, but checking whether the code actually does what it claims to.

Ran it on 4 production codebases from public GitHub repos (built with various AI tools). Average health score: 40/100. Found 21 critical bugs that compilers and linters missed entirely — things like admin routes with no auth guards, analytics dashboards backed by hardcoded arrays, and API endpoints the frontend calls that don't exist in the backend.

The most interesting finding: the common fix approaches don't work well. Self-refine (asking the AI to try again) barely moves the needle (~87% on HumanEval). LLM-as-judge actually *regresses* after 5 iterations — the reviewer introduces false positives and the generator "fixes" correct code.

Formal verification is the only method that converges monotonically. We hit 100% on HumanEval at k=3 iterations.

Full data and methodology here: https://trylucid.dev/report (peer-reviewed, DOI: 10.5281/zenodo.18522644)

If anyone wants to run their Cursor projects through the verification pipeline, happy to do it for free. Genuinely curious how Cursor-assisted code compares — we haven't benchmarked it yet and I'd expect it to be different given the IDE workflow.

---

## 2. Bolt.new Discord

**Channel:** #showcase or #general

---

**Ran formal verification on AI-built apps — sharing findings**

Built a verification system that checks whether AI-generated code actually does what it says. Not linting — semantic verification. Does the auth guard actually guard? Does the data actually persist? Does the API endpoint actually exist?

Tested it on 4 real-world apps from public GitHub repos built with AI coding tools. Findings:

- Average health score: 40/100
- 21 critical bugs across all apps
- Most common patterns: missing backend implementations, auth components that are imported but never wired up, analytics showing hardcoded data

Simple apps (todo lists) score 87-100%. The gap shows up with real complexity — multiple integrations, auth flows, data persistence.

On the methodology side: we benchmarked different verification approaches on HumanEval (164 tasks). Self-refine: ~87%. LLM-as-judge: starts at 98% but drops to 97% with more iterations. Formal verification: 100% at k=3, never regresses.

Report with full data: https://trylucid.dev/report

Happy to run anyone's Bolt project through the pipeline for free. Takes about 5 minutes. You get a claim-by-claim breakdown of what's verified vs. what's broken, with fix suggestions. DM me a repo link if interested.

---

## 3. Replit Discord

**Channel:** #showcase or #community

---

**Findings from running formal verification on AI-generated code**

I built a tool (LUCID) that does semantic verification on codebases — checks whether features actually work, not just whether they compile. Things like: does this API endpoint exist? Is this data real or hardcoded? Does auth actually block unauthorized access?

Some patterns I keep seeing in AI-built apps:

1. **Scaffolding as features** — UI renders beautifully, but the backend logic is either missing or returns mock data. Charts labeled "Real-time insights" backed by static arrays.

2. **Import-but-don't-implement** — Auth guards, error boundaries, and API clients get imported but never properly wired up. The code references them, so it reads like they work.

3. **Silent fallbacks** — When an API call fails, the app falls back to default data instead of showing an error. It *looks* like it works.

Across 4 production apps from public GitHub repos, average health score was 40/100 with 21 critical bugs. On HumanEval (164 coding tasks), formal verification hits 100% — the only approach that converges without regressing.

Full report: https://trylucid.dev/report

If anyone wants me to run their Replit project through verification, happy to do it for free. You get a detailed report of what's actually working vs. what's scaffolded. DM me.

---

## Posting Guidelines

- **Do NOT lead with LUCID as a product.** Lead with findings and patterns.
- **Offer free verification** as genuine helpfulness, not a sales pitch.
- **Respond to comments** with data, not marketing.
- **If someone pushes back on methodology**, point to the DOI and offer to discuss specifics.
- **Never compare platforms negatively** in their own communities. Share general patterns.
- **Check each server's self-promotion rules** before posting.
