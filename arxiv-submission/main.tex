\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{url}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{float}
\usepackage{array}
\usepackage{tabularx}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

% Title
\title{LUCID: Leveraging Unverified Claims Into Deliverables\\[0.5em]
\large A Neuroscience-Grounded Framework for Exploiting Large Language Model\\Hallucination as a Software Specification Engine}

\author{Ty Wells\\
\texttt{ty@snapperland.com}\\
\url{https://github.com/gtsbahamas/hallucination-reversing-system}
}

\date{February 2026}

\begin{document}

\maketitle

% ============================================================
% ABSTRACT
% ============================================================
\begin{abstract}
Large language model (LLM) hallucination is universally treated as a defect to be minimized. We argue this framing is backwards. Hallucination---the confident generation of plausible but unverified claims---is computationally identical to the brain's pattern completion mechanism that underlies both perception and imagination. We present LUCID (Leveraging Unverified Claims Into Deliverables), a development methodology that deliberately invokes LLM hallucination, extracts the resulting claims as testable requirements, verifies them against a real codebase, and iteratively converges hallucinated fiction toward verified reality. By prompting an LLM to author Terms of Service for an application that does not yet exist, we exploit the model's confabulatory tendency to produce comprehensive, precise, multi-dimensional specifications---covering functionality, security, data privacy, performance, and legal compliance---in seconds. We provide theoretical grounding through three convergent lines of evidence: (1) the mathematical equivalence between transformer attention and hippocampal pattern completion, (2) the predictive processing framework from cognitive neuroscience, and (3) the REBUS model of psychedelic hallucination. We demonstrate the framework on a real-world application, achieving convergence from 57.3\% to 90.8\% compliance across six iterations. We position LUCID as the software engineering analogue of protein hallucination \citep{anishchenko2021}, where neural network ``dreams'' serve as blueprints validated against physical reality. Formal impossibility results proving that hallucination cannot be eliminated from LLMs \citep{xu2024,banerjee2024,karpowicz2025} suggest that harnessing hallucination may be more productive than fighting it.
\end{abstract}

\textbf{Keywords:} LLM hallucination, confabulation, predictive processing, specification generation, requirements engineering, iterative convergence, neuroscience of hallucination

% ============================================================
% 1. INTRODUCTION
% ============================================================
\section{Introduction}

The field of large language model research has, since the publication of GPT-3 \citep{brown2020}, treated hallucination as a failure mode requiring mitigation. Retrieval-Augmented Generation \citep{lewis2020}, Chain-of-Thought prompting \citep{wei2022}, Chain-of-Verification \citep{dhuliawala2024}, and Constitutional AI \citep{bai2022} all share a common goal: reduce the rate at which models generate false, ungrounded, or fabricated content.

This work takes the opposite position. We argue that:

\begin{enumerate}
    \item \textbf{Hallucination is mathematically inevitable} in any model that generalizes beyond its training distribution \citep{xu2024,banerjee2024,karpowicz2025}.
    \item \textbf{Hallucination is computationally equivalent} to the pattern completion mechanism that underlies human perception, memory, and imagination \citep{ramsauer2020,clark2023,friston2010}.
    \item \textbf{Hallucination, when deliberately invoked and externally verified, is a productive signal}---the richest, cheapest, and fastest method available for generating comprehensive software specifications.
\end{enumerate}

We present LUCID, a six-phase iterative methodology:

\begin{center}
\textbf{Describe} $\rightarrow$ \textbf{Hallucinate} $\rightarrow$ \textbf{Extract} $\rightarrow$ \textbf{Build} $\rightarrow$ \textbf{Converge} $\rightarrow$ \textbf{Regenerate}
\end{center}

The key innovation is Phase 2: we ask an LLM to write a Terms of Service document for an application that does not exist. The model confabulates---inventing specific capabilities, data handling procedures, performance guarantees, user rights, and limitations---in the precise, declarative language that legal documents demand. Each confabulated claim becomes a testable requirement. Verification against the actual codebase reveals which claims are real (the model guessed correctly), which are aspirational (plausible but unimplemented), and which are infeasible (should be dropped). Regeneration feeds verified reality back to the model, producing an updated specification that is progressively more grounded with each iteration.

This paper makes four contributions:

\begin{enumerate}
    \item A \textbf{formal framework} (LUCID) for exploiting LLM hallucination as a specification engine, with a working open-source implementation.
    \item \textbf{Theoretical grounding} from cognitive neuroscience, connecting LLM hallucination to predictive processing, memory reconsolidation, confabulation, and lucid dreaming.
    \item \textbf{Empirical results} demonstrating convergence from 57.3\% to 90.8\% specification-reality alignment across six iterations on a production application.
    \item A \textbf{positioning argument} that LUCID is the software engineering analogue of protein hallucination---the Nobel Prize-winning methodology where neural network ``dreams'' serve as blueprints for novel biological structures \citep{anishchenko2021}.
\end{enumerate}

% ============================================================
% 2. BACKGROUND AND RELATED WORK
% ============================================================
\section{Background and Related Work}

\subsection{The Impossibility of Eliminating Hallucination}

Three independent formal results establish that hallucination is intrinsic to LLMs:

\citet{xu2024} prove, using learning theory, that any LLM with a computable ground truth function will inevitably hallucinate when used as a general problem solver. The proof shows that LLMs cannot learn all computable functions, and therefore must sometimes generate outputs inconsistent with ground truth.

\citet{banerjee2024} reach the same conclusion via G\"{o}del's First Incompleteness Theorem, the Halting Problem, and the Emptiness Problem. They demonstrate that hallucination stems from the fundamental mathematical and logical structure of LLMs, and cannot be eliminated through architectural improvements, dataset enhancements, or fact-checking mechanisms.

Most recently, \citet{karpowicz2025} proved a fundamental impossibility theorem: no LLM performing non-trivial knowledge aggregation can simultaneously achieve truthful knowledge representation, semantic information conservation, complete revelation of relevant knowledge, and knowledge-constrained optimality. The proof draws on mechanism design (Green-Laffont theorem), proper scoring rules (Savage), and transformer architecture analysis (log-sum-exp convexity), establishing that the impossibility stems from the mathematical structure of information aggregation itself, not engineering limitations.

These results motivate our core argument: if hallucination cannot be eliminated, a productive methodology must \emph{harness} it rather than fight it.

\subsection{Self-Correction Requires External Feedback}

A critical finding from \citet{huang2024} demonstrates that LLMs cannot reliably self-correct their reasoning without external feedback. Performance can \emph{degrade} after self-correction attempts. The bottleneck is feedback generation: LLMs cannot produce reliable signals about their own errors.

This result has direct implications for LUCID's design. The verification phase uses the actual codebase---not the model's self-assessment---as ground truth. This aligns with the LLM-Modulo framework \citep{kambhampati2024}, which pairs LLMs with external sound verifiers and achieves dramatic accuracy improvements (24\% $\rightarrow$ 98\% on blocks world planning).

\subsection{Hallucination as Productive Signal}

Several lines of work have begun to reframe hallucination as useful:

\textbf{Protein hallucination.} \citet{anishchenko2021} used the trRosetta neural network to iteratively optimize random amino acid sequences via Monte Carlo sampling---a process the authors called ``hallucination.'' The hallucinated protein structures, when expressed in bacteria, closely matched predictions. David Baker's subsequent work generated millions of novel proteins, leading to approximately 100 patents and 20+ biotech companies, and the 2024 Nobel Prize in Chemistry.

\textbf{Drug discovery.} \citet{halludrug2025} found that hallucinations significantly improve predictive accuracy for molecule property prediction by acting as ``implicit counterfactuals''---speculative interpretations that help LLMs generalize over unseen compounds.

\textbf{Confabulation value.} \citet{sui2024} empirically demonstrated that LLM confabulations display increased narrativity and semantic coherence relative to veridical outputs, mirroring the human propensity to use narrativity as a cognitive resource for sense-making.

\textbf{Computational imagination.} \citet{pip2025} deliberately amplified LLM hallucinations using LoRA fine-tuning for creative applications, reframing hallucinations as ``computational imagination.''

\subsection{Closed-Loop Verification Systems}

LUCID builds on established generate-verify-refine architectures. Chain-of-Verification \citep{dhuliawala2024} implements a four-step draft-verify-revise process, reducing factual hallucinations by 50--70\%. CRITIC \citep{gou2023} introduces tool-interactive critiquing where LLMs interact with external tools to verify and correct output iteratively. Self-Refine \citep{madaan2023} demonstrated the generate-critique-refine loop. VENCE \citep{chen2023}, whose title ``Converge to the Truth'' anticipates our convergence framing, formulates factual error correction as iterative constrained editing.

LUCID differs from all of these in two respects: (1) verification occurs against a \emph{codebase}, not against web knowledge or reference documents, and (2) the output is not a corrected text but a \emph{specification} that drives development.

\subsection{Adjacent Methodologies}

\begin{table}[H]
\centering
\small
\begin{tabularx}{\textwidth}{>{\raggedright\arraybackslash}p{3cm} >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}p{1.5cm} >{\raggedright\arraybackslash}p{2cm} >{\raggedright\arraybackslash}p{1.5cm}}
\toprule
\textbf{Methodology} & \textbf{Specification Source} & \textbf{AI Role} & \textbf{Hallucination} & \textbf{Loop} \\
\midrule
Spec-Driven Dev & Human-authored spec & Implements & Prevents & No \\
Readme-Driven Dev & Human-authored README & None & N/A & No \\
Design Fiction & Human-authored fiction & Optional & Intentional (human) & Loose \\
Protein Halluc. & Neural network output & Generates & Exploits & Validate \\
Vibe Coding & Human prompt & Generates & Tolerates & No \\
\textbf{LUCID} & \textbf{AI-hallucinated ToS} & \textbf{Hallucinates, extracts, verifies} & \textbf{Exploits} & \textbf{Yes} \\
\bottomrule
\end{tabularx}
\caption{Comparison of specification methodologies. LUCID is the only approach combining AI-generated specification, deliberate hallucination exploitation, and iterative convergence verification against a codebase.}
\label{tab:comparison}
\end{table}

% ============================================================
% 3. THEORETICAL FOUNDATIONS
% ============================================================
\section{Theoretical Foundations}

We ground LUCID in three convergent lines of evidence from cognitive neuroscience and mathematical machine learning.

\subsection{Transformers as Associative Memory}

\citet{ramsauer2020} proved that transformer self-attention is mathematically equivalent to the update rule of modern Hopfield networks---associative memory systems that retrieve stored patterns from partial cues. Given a query (partial cue), the attention mechanism retrieves the most similar stored pattern (key-value pair). This is pattern completion: the same computation performed by the hippocampal CA3 autoassociative network.

\citet{whittington2021} extended this connection, showing that transformer architectures relate to hippocampal formation computations, specifically place cells and grid cells used for spatial memory and navigation.

The implication is direct: when an LLM generates text about a nonexistent application, it is performing pattern completion from partial cues (the prompt) against distributed representations (training data). The output includes both veridical completions (patterns the model has reliably encoded) and confabulated completions (plausible extensions that overshoot the stored patterns). This is identical to what the hippocampus does when reconstructing a memory from a partial cue---some details are accurate recall, and some are gap-filling confabulation \citep{bartlett1932,loftus2005}.

\subsection{Predictive Processing: Perception as Controlled Hallucination}

The dominant framework in modern cognitive neuroscience---\emph{predictive processing}---holds that the brain is fundamentally a prediction machine \citep{friston2009,friston2010,clark2013,hohwy2013,seth2021}.

The brain does not passively receive sensory data. It generates top-down predictions about what it expects to perceive, compares those predictions against incoming sensory signals, and propagates only the \textbf{prediction error} upward through the cortical hierarchy. When predictions are good, experience feels normal. When predictions are unconstrained by sensory data, the result is hallucination.

As \citet{seth2021} states: ``We're all hallucinating all the time; when we agree about our hallucinations, we call it reality.'' \citet{clark2023} formalizes this: ``Perception is controlled hallucination. Hallucination is uncontrolled perception.'' Both use the same generative machinery; the difference is the degree of constraint from external evidence.

This framework maps precisely onto LLM behavior:

\begin{table}[H]
\centering
\small
\begin{tabularx}{\textwidth}{>{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X}
\toprule
\textbf{Predictive Processing} & \textbf{LLM Generation} \\
\midrule
Internal generative model & Trained transformer weights \\
Top-down prediction & Next-token probability distribution \\
Sensory data constraining predictions & Context window, RAG, grounding \\
Prediction error signal & Loss during training \\
Hallucination (unconstrained) & Generation without factual grounding \\
Perception (constrained) & Generation with strong context/retrieval \\
\bottomrule
\end{tabularx}
\caption{Mapping between the predictive processing framework and LLM generation.}
\label{tab:predictive}
\end{table}

LUCID deliberately operates in the ``unconstrained'' mode during the Hallucinate phase, then progressively introduces constraint during Converge and Regenerate. Each iteration moves the system along the spectrum from hallucination toward perception.

\subsection{REBUS: The Psychedelic Model and Temperature}

\citet{carthharris2019} proposed the REBUS model (Relaxed Beliefs Under Psychedelics), integrating predictive coding with the entropic brain hypothesis \citep{carthharris2014}. Psychedelics reduce the \textbf{precision weighting} of high-level priors---the brain's top-down constraints. When these constraints relax, novel associations form that are normally suppressed, producing both hallucination and creative insight.

This maps directly to the \textbf{temperature parameter} in LLM sampling. High precision (brain) or low temperature (LLM) yields conservative, factual output. Low precision or high temperature yields divergent, hallucination-prone generation. The REBUS model explains why psychedelic states are associated with both hallucination \emph{and} creativity---relaxing constraints enables novel pattern combinations. LUCID exploits this: the Hallucinate phase operates at ``high temperature,'' and the convergence loop progressively increases ``precision.''

\subsection{Confabulation: The Correct Term}

Multiple authors have argued that \emph{confabulation}---not hallucination---is the correct term for LLM fabrication. \citet{smith2023} use neuroanatomical metaphor to argue that LLMs behave like an ``unmitigated confabulating left hemisphere.'' \citet{hirstein2005} identified the key structural insight: the creative ability to construct plausible responses and the ability to \emph{check} them are \textbf{separate processes} in the brain. Confabulation occurs when the checking process fails. LLMs have the generative process but lack the checker. LUCID reintroduces the checker via external codebase verification.

\citet{schnider2003} identified the neural substrate: the posterior medial orbitofrontal cortex performs \textbf{reality filtering}---suppressing activated memories that do not pertain to ongoing reality. LUCID's verification step serves as the computational analogue of orbitofrontal reality filtering.

\subsection{Memory Reconsolidation}

\citet{loftus2005} demonstrated that human memory is reconstructive, not reproductive. When recalling an event, the brain regenerates it from partial traces, filling gaps with plausible details. The mechanism is hippocampal pattern completion \citep{yassa2011}: given a partial cue, the CA3 autoassociative network fires the full stored pattern. Since transformer self-attention is mathematically equivalent to this computation \citep{ramsauer2020}, the analogy between ``a trickle of memory triggering hallucination'' and LLM next-token prediction is not metaphorical---it describes the same underlying computation in different substrates.

\subsection{Lucid Dreaming: The System's Namesake}

The name ``LUCID'' embodies the neuroscience of \emph{lucid dreaming}---the state where a dreamer becomes metacognitively aware that they are dreaming while remaining in the dream \citep{baird2019,filevich2015}. The dorsolateral prefrontal cortex (executive function, reality monitoring) reactivates, and the dreamer can \emph{steer} the dream without stopping it.

\begin{table}[H]
\centering
\small
\begin{tabularx}{\textwidth}{>{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X}
\toprule
\textbf{Lucid Dreaming} & \textbf{LUCID System} \\
\midrule
The dream (unconstrained generation) & Hallucinated Terms of Service \\
Becoming lucid (recognizing the dream) & Extract phase (claims become hypotheses) \\
Reality testing (checking dream vs.\ reality) & Verify phase (claims checked against code) \\
Dream steering (directing content) & Regenerate phase (feeding reality back) \\
Prefrontal cortex reactivation & Human judgment evaluating verdicts \\
Staying in the dream while aware & Staying in the generative loop while verifying \\
\bottomrule
\end{tabularx}
\caption{Mapping between lucid dreaming neuroscience and the LUCID system.}
\label{tab:lucid}
\end{table}

A lucid dreamer does not fight the dream. They participate with awareness, harvesting creative content while maintaining the ability to distinguish generated from real. LUCID does exactly this to AI hallucination.

This also maps onto dual-process theory \citep{kahneman2011}: LLMs are pure System~1 (fast, automatic, confabulation-prone). LUCID wraps a System~1 machine in a System~2 process. The convergence loop \emph{is} the deliberate, checking function that the model lacks.

% ============================================================
% 4. THE LUCID FRAMEWORK
% ============================================================
\section{The LUCID Framework}

\subsection{Overview}

LUCID is a six-phase iterative cycle that converts loose application descriptions into verified software specifications through controlled exploitation of LLM hallucination.

\begin{enumerate}
    \item \textbf{Describe.} Provide a deliberately incomplete application description. Gaps are where confabulation does its most productive work.
    \item \textbf{Hallucinate.} The LLM writes Terms of Service as if the application is live in production. The prompt instructs declarative statements (``The Service processes X'' rather than ``may process'').
    \item \textbf{Extract.} Each declarative claim is parsed into a structured requirement with category (functionality, security, data-privacy, operational, legal), severity (critical, high, medium, low), and testability flag.
    \item \textbf{Build.} Implementation proceeds using any methodology. ToS-derived claims serve as acceptance criteria.
    \item \textbf{Converge.} Two-step verification: (a) file selection---identify which source files contain evidence for each claim; (b) verdict assignment---PASS, PARTIAL, FAIL, or N/A with evidence.
    \item \textbf{Regenerate.} Verified reality is fed back. The model writes updated ToS retaining PASS claims, revising PARTIAL claims, dropping or revising FAIL claims, and hallucinating new capabilities.
\end{enumerate}

\subsection{Why Terms of Service}

ToS is the ideal hallucination vehicle because the document format demands specificity across multiple dimensions simultaneously. Service descriptions produce functional requirements; acceptable use policies produce input validation rules; data handling sections produce privacy and security requirements; limitation sections produce performance boundaries; SLA sections produce reliability requirements; termination sections produce lifecycle requirements; liability sections produce error handling requirements.

No other document format forces this level of specificity across this many dimensions. Legal language cannot be vague---``The Service may do things'' is not a valid legal clause---so the format forces the model to hallucinate \emph{precisely}. A typical hallucination produces 400--600 lines of dense legal text containing 80--150 extractable claims.

\subsection{Compliance Score}

Verification assigns verdicts, and compliance is computed as:

\begin{equation}
S = \frac{N_{\text{pass}} + 0.5 \cdot N_{\text{partial}}}{N_{\text{total}} - N_{\text{na}}} \times 100
\end{equation}

where $N_{\text{pass}}$, $N_{\text{partial}}$, $N_{\text{total}}$, and $N_{\text{na}}$ are the counts of PASS, PARTIAL, total, and N/A verdicts respectively.

\subsection{Convergence Dynamics}

Each iteration shifts the ratio of accurate-to-hallucinated claims. PASS claims are retained as verified. New hallucinations are generated in context of verified capabilities, making them more grounded. The gap between specification and reality shrinks monotonically (see Section~\ref{sec:results}).

% ============================================================
% 5. IMPLEMENTATION
% ============================================================
\section{Implementation}

LUCID is implemented as an open-source CLI tool in TypeScript (Node.js 20+), using the Anthropic Claude SDK for all LLM interactions. The tool provides commands for each phase: \texttt{lucid hallucinate}, \texttt{lucid extract}, \texttt{lucid verify}, \texttt{lucid report}, \texttt{lucid remediate}, and \texttt{lucid regenerate}. All artifacts are stored in \texttt{.lucid/iterations/\{N\}/} directories, maintaining a complete audit trail.

Claim extraction uses streaming API calls with validation against strict type schemas. Codebase verification uses a two-step process: file selection (model identifies relevant files from the file tree) followed by verdict assignment (model reads file contents truncated to 10K characters each, 100K total context). Claims are processed in batches of 15.

Failed and partial verifications are transformed into actionable remediation tasks with title, description, action type (add/modify/remove/configure), target files, estimated effort, and code-level guidance.

% ============================================================
% 6. EMPIRICAL RESULTS
% ============================================================
\section{Empirical Results}
\label{sec:results}

\subsection{Case Study}

We applied LUCID to a production Next.js application---a career development platform with AI coaching, financial planning, goal tracking, and document management---comprising approximately 30,000 lines of TypeScript across 200+ files.

\subsection{Convergence Data}

\begin{table}[H]
\centering
\begin{tabular}{lcccccc}
\toprule
\textbf{Iteration} & \textbf{Score} & \textbf{Claims} & \textbf{PASS} & \textbf{PARTIAL} & \textbf{FAIL} & \textbf{N/A} \\
\midrule
1 & $\sim$35\% (est.) & 91 & --- & --- & --- & --- \\
3 & 57.3\% & 91 & 38 & 15 & 32 & 6 \\
4 & 69.8\% & 91 & 47 & 18 & 20 & 6 \\
5 & 83.2\% & 91 & 61 & 15 & 9 & 6 \\
6 & 90.8\% & 91 & 68 & 12 & 5 & 6 \\
\bottomrule
\end{tabular}
\caption{Convergence across iterations. Compliance score increased monotonically.}
\label{tab:convergence}
\end{table}

After six iterations, five claims remained as FAIL: weekly job market data refresh (hardcoded), subscription downgrade retention logic, ClamAV malware scanning, server-side rate limiting, and account lockout parameter mismatch. All five represent genuine missing functionality---not false positives.

\subsection{Verification Layers (Iteration 6)}

\begin{table}[H]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Layer} & \textbf{Score} \\
\midrule
Code-level verification & 84.5\% \\
End-to-end testing & 100\% \\
UX audit & 85\% \\
Click verification & 95\% \\
\textbf{Composite} & \textbf{90.8\%} \\
\bottomrule
\end{tabular}
\caption{Multi-layer verification scores at iteration 6.}
\label{tab:layers}
\end{table}

\subsection{Token Economics}

A complete six-iteration cycle cost approximately \$17 in API tokens---producing a verified specification with 91 claims, a gap report, and a prioritized remediation plan. Individual phases ranged from \$0.15 (hallucination) to \$1.50 (verification per iteration).

% ============================================================
% 7. DISCUSSION
% ============================================================
\section{Discussion}

\subsection{Why LUCID Works}

LUCID's effectiveness can be explained through three converging mechanisms:

\textbf{Pattern completion from training data.} Given a vague application description, the model completes the pattern using representations learned from millions of ToS documents, software documentation sets, and codebases. The completions reflect genuine statistical regularities---real applications in this domain \emph{tend} to have these features.

\textbf{Legal language as forcing function.} The ToS format constrains the model to produce specific, testable, declarative claims. A prompt asking for ``features'' yields vague bullets. A prompt asking for legally binding commitments yields precise specifications.

\textbf{External verification as reality filtering.} Following \citeauthor{schnider2003}'s (\citeyear{schnider2003}) model of orbitofrontal reality filtering, the verification step suppresses hallucinated claims that do not correspond to reality. This is the System~2 checking process the model lacks \citep{kahneman2011}, and that \citet{huang2024} proved cannot be performed by the model itself.

\subsection{Relationship to Protein Hallucination}

The closest existing analogue to LUCID is Baker Lab's protein hallucination \citep{anishchenko2021}. The structural parallel is exact: neural network generates novel structures that do not exist; laboratory/codebase validates them; functional results become candidates; iterative sampling refines candidates. Baker's methodology produced approximately 100 patents and 20+ biotech companies. The insight---that neural network ``dreams'' can serve as blueprints for real-world engineering---earned the 2024 Nobel Prize in Chemistry. LUCID applies the identical insight to software engineering.

\subsection{The Lucid Dreaming Design Principle}

The neuroscience of lucid dreaming provides a design principle, not just a metaphor: (1)~\emph{Do not wake up}---do not suppress hallucination; (2)~\emph{Gain metacognitive awareness}---the Extract phase parses claims as unverified hypotheses; (3)~\emph{Reality-test within the dream}---verify against code; (4)~\emph{Steer, don't control}---provide context and let the model hallucinate freely within it.

\subsection{Beyond Hallucination Reduction}

The deeper claim is that LUCID is not a hallucination \emph{reduction} technique. It is a specification \emph{extraction} technique that uses hallucination as input signal. No human requirements gathering process produces 91 testable claims spanning functionality, security, data privacy, performance, operations, and legal compliance in 30 seconds. The hallucination does.

\subsection{Toward One-Shot Development}

LUCID is not one-shot; it is a convergence loop. But neither is perception---perception is a rapid inference loop (predict $\rightarrow$ compare $\rightarrow$ update) that runs so fast it feels instantaneous \citep{friston2009}. The trajectory toward one-shot is determined by initial hallucination quality (improves with better models) and verification speed (improves with better tooling). At the limit, a sufficiently good model with sufficiently fast verification converges in a single iteration.

\subsection{Limitations}

LUCID's claim quality depends on the underlying model's training data. The verification step uses an LLM (not formal verification), introducing potential for verification errors. LUCID is designed for software applications; applicability to other engineering domains is untested. Hallucinated ToS documents should not be used as legal documents without qualified review.

% ============================================================
% 8. FUTURE WORK
% ============================================================
\section{Future Work}

\textbf{Multi-document hallucination.} API documentation, user manuals, privacy policies, and compliance certifications each force different kinds of specificity. Simultaneous multi-document hallucination with cross-document consistency verification may improve specification coverage.

\textbf{Formal verification integration.} Replacing LLM-based verification with property-based testing, model checking, or static analysis for specific claim categories would increase fidelity.

\textbf{Continuous monitoring.} Integration into CI/CD pipelines would enable real-time specification drift detection on each code push.

\textbf{Cross-domain transfer.} The core insight---hallucination as blueprint, verification as reality filter---may transfer to regulatory compliance, safety certifications, and standards conformance.

\textbf{Hallucination quality benchmarks.} The initial compliance score (before remediation) may serve as a metric for model capability---a ``specification hallucination benchmark.''

% ============================================================
% 9. CONCLUSION
% ============================================================
\section{Conclusion}

We have presented LUCID, a framework that inverts the dominant paradigm around LLM hallucination. Rather than treating hallucination as a defect, LUCID exploits it as a specification engine---the fastest, cheapest, most comprehensive method available for generating testable software requirements from minimal input.

The theoretical grounding is robust. Transformer attention is mathematically equivalent to hippocampal pattern completion \citep{ramsauer2020}. The predictive processing framework establishes that hallucination and perception are the same generative computation under different constraint conditions \citep{clark2023,seth2021,friston2010}. The REBUS model explains why relaxing constraints enables creative discovery \citep{carthharris2019}. The confabulation literature provides the structural insight that generation and verification are separable processes \citep{hirstein2005,schnider2003}---LLMs have the former but not the latter.

LUCID provides the latter. By verifying hallucinated claims against real codebases and feeding verified reality back into the generative process, LUCID creates a convergence loop that progressively transforms hallucinated fiction into verified specification. Empirical results demonstrate convergence from 57.3\% to 90.8\% compliance across six iterations at a cost of approximately \$17 in API tokens.

The formal impossibility results of \citet{xu2024}, \citet{banerjee2024}, and \citet{karpowicz2025} establish that hallucination cannot be eliminated. If hallucination is inevitable, the productive response is to harness it. LUCID is one such harness---and the precedent of protein hallucination earning the 2024 Nobel Prize suggests that the principle of ``neural network dreams as engineering blueprints'' has transformative potential across domains.

We leave the reader with a reformulation of Seth's dictum:

\begin{quote}
\emph{Normal specification is hallucination constrained by reality. LUCID is the first development methodology that uses this principle: generate freely, then constrain iteratively, just as the brain does.}
\end{quote}

% ============================================================
% REFERENCES
% ============================================================
\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
